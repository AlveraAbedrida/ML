{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Step_4_5_Multivariate Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlveraAbedrida/ML/blob/master/Copy_of_Step_4_5_Multivariate_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cn_fXW__64b",
        "colab_type": "text"
      },
      "source": [
        "### Multivariate Regression\n",
        "\n",
        "In most of the real life use cases there will be more than one independent variable, so the concept of having multiple independent variables is called as multivariate regression. The equation take the below form.\n",
        "\n",
        "y = m_1 x_1 + m_2 x_2 + m_3 x_3 + . . .+ m_n x_n\n",
        "Where, each independent variable is represented by x’s, and m’s are the corresponding coefficients.\n",
        "\n",
        "We’ll be using the ‘statsmodels’ Python library to learn the basics of multivariate regression, as it provide more useful statistics results which is helpful from learning perspective. Once you understand the fundamental concepts, you can either use ‘scikit-learn’ or ‘statsmodels’ package as both are efficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0XbAUvjGJ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqKcym7YGKkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyDrive\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "class download_data_from_folder(object):\n",
        "    def __init__(self,path):\n",
        "        path_id = path[path.find('id=')+3:]\n",
        "        self.file_list = self.get_files_in_location(path_id)\n",
        "        self.unwrap_data(self.file_list)\n",
        "    def get_files_in_location(self,folder_id):\n",
        "        file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "        return file_list\n",
        "    def unwrap_data(self,file_list,directory='.'):\n",
        "        for i, file in enumerate(file_list):\n",
        "            print(str((i + 1) / len(file_list) * 100) + '% done copying')\n",
        "            if file['mimeType'].find('folder') != -1:\n",
        "                if not os.path.exists(os.path.join(directory, file['title'])):\n",
        "                    os.makedirs(os.path.join(directory, file['title']))\n",
        "                print('Copying folder ' + os.path.join(directory, file['title']))\n",
        "                self.unwrap_data(self.get_files_in_location(file['id']), os.path.join(directory, file['title']))\n",
        "            else:\n",
        "                if not os.path.exists(os.path.join(directory, file['title'])):\n",
        "                    downloaded = drive.CreateFile({'id': file['id']})\n",
        "                    downloaded.GetContentFile(os.path.join(directory, file['title']))\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPbTPhWTGPwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'https://drive.google.com/open?id=13hFQ09ptYr-Ud5xOJ0Xx4cV0akc1RnZw'\n",
        "download_data_from_folder(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyWqycxI_64c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf4_HRXu_64h",
        "colab_type": "text"
      },
      "source": [
        "### Housing data\n",
        "\n",
        "\n",
        "\n",
        "The housing data contains sales prices of houses in the city of Windsor. Below is the brief description about each variables.\n",
        "- price: sale price of a house\n",
        "- lotsize: the lot size of a property in square feet\n",
        "- bedrooms: number of bedrooms\n",
        "- bathrms: number of full bathrooms\n",
        "- stories: number of stories excluding basement\n",
        "- driveway: does the house has a driveway ?\n",
        "- recroom: does the house has a recreational room ?\n",
        "- fullbase: does the house has a full finished basement ?\n",
        "- gashw: does the house uses gas for hot water heating ?\n",
        "- airco: does the house has central air conditioning ?\n",
        "- garagepl: number of garage places\n",
        "- prefarea: is the house located in the preferred neighbourhood of the city ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tT0HIV_64i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "df = pd.read_csv('Housing_Modified.csv')\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WunAepB_64o",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "Let's build a model to predict the house price (dependent variable), considering rest of the variables as independent variables\n",
        "\n",
        "Handling categorical variables\n",
        "- Label Binarizer: This will replace the binary variable text with numeric vales. Lets use this function for the below binary variables of our current data set\n",
        "    - driveway\n",
        "    - recroom\n",
        "    - fullbase\n",
        "    - gashw\n",
        "    - airco\n",
        "    - prefarea\n",
        "\n",
        "- Label Encoder: This will replace category level with number representation\n",
        "\n",
        "- One Hot Encoder: This will convert n levels to n-1 new variable, and the new variables will use 1 to indicate the presence of level and 0 for otherwise. Note that before calling OneHotEncoder, we should use LabelEncoder to convert levels to number. Alternatively we can achieve the same using get_dummies of pandas package. This is much efficient to use as we can directly use it on the column with text description without having to convert to numbers first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdkU699H_64o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert binary fields to numeric boolean fields\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "\n",
        "df.driveway = lb.fit_transform(df.driveway)\n",
        "df.recroom = lb.fit_transform(df.recroom)\n",
        "df.fullbase = lb.fit_transform(df.fullbase)\n",
        "df.gashw = lb.fit_transform(df.gashw)\n",
        "df.airco = lb.fit_transform(df.airco)\n",
        "df.prefarea = lb.fit_transform(df.prefarea)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_I5Oai6_64s",
        "colab_type": "text"
      },
      "source": [
        "### Create dummy variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EczGybvK_64t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dummy variables for stories\n",
        "df_stories = pd.get_dummies(df['stories'], prefix='stories', drop_first=True)\n",
        "\n",
        "# Join the dummy variables to the main dataframe\n",
        "df = pd.concat([df, df_stories], axis=1)\n",
        "del df['stories']\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffqZAWQz_64w",
        "colab_type": "text"
      },
      "source": [
        "### Correlation plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7_tPDZb_64x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets plot correlation matrix using statmodels graphics packages's plot_corr\n",
        "\n",
        "# create correlation matrix\n",
        "corr = df.corr()\n",
        "sm.graphics.plot_corr(corr, xnames=list(corr.columns))\n",
        "plt.show()\n",
        "\n",
        "print(corr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI_M-ycFJIq0",
        "colab_type": "text"
      },
      "source": [
        "We can see from the plot that stories_one has a strong negative correlation with\n",
        "stories_two. Let’s perform the VIF analysis to eliminate strongly correlated independent\n",
        "variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6NEK3Vn_640",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatively we can use seaborn package\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(df.corr(), vmax=.8, square=True,annot=True,fmt='.1f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ure85Eny_646",
        "colab_type": "text"
      },
      "source": [
        "### Variation Inflation Factor\n",
        "\n",
        "VIF value of greater than 10 is a indicator of multicollinearity, and these variables should be excluded from the regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I9hV4fo_646",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a Python list of feature names\n",
        "independent_variables = ['lotsize', 'bedrooms', 'bathrms','driveway', 'recroom',\n",
        "                         'fullbase','gashw','airco','garagepl', 'prefarea',\n",
        "                        'stories_one','stories_two','stories_three']\n",
        "\n",
        "# use the list to select a subset from original DataFrame\n",
        "X = df[independent_variables]\n",
        "y = df['price']\n",
        "\n",
        "thresh = 10\n",
        "\n",
        "for i in np.arange(0,len(independent_variables)):\n",
        "    vif = [variance_inflation_factor(X[independent_variables].values, ix) for ix in range(X[independent_variables].shape[1])]\n",
        "    maxloc = vif.index(max(vif))\n",
        "    if max(vif) > thresh:\n",
        "        print(\"vif :\", vif)\n",
        "        print('dropping \\'' + X[independent_variables].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
        "        del independent_variables[maxloc]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print('Final variables:', independent_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqPeuYhkJo_-",
        "colab_type": "text"
      },
      "source": [
        "We can see that VIF analysis has eliminated bedrooms greater than 10; however,\n",
        "stories_one and stories_two have been retained.\n",
        "Let’s run the first iteration of a multivariate regression model with the set of\n",
        "independent variables that have passed the VIF analysis.\n",
        "To test the model performance, the common practice is to split the data set into\n",
        "80/20 (or 70/30) for train/test, respectively, and use the train data set to build the model.\n",
        "Then apply the trained model on the test data set to evaluate the performance of the\n",
        "model ("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWx3oXmC_64-",
        "colab_type": "text"
      },
      "source": [
        "### Run first iteration of model\n",
        "\n",
        "We split the data into train/test of 80/20 percent respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZ9oTX-_64_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the list to select a subset from original DataFrame\n",
        "X = df[independent_variables]\n",
        "y = df['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1)\n",
        "\n",
        "# create a fitted model\n",
        "lm = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# print the summary\n",
        "print(lm.summary())\n",
        "\n",
        "# make predictions on the testing set\n",
        "y_train_pred = lm.predict(X_train)\n",
        "y_test_pred = lm.predict(X_test)\n",
        "y_pred = lm.predict(X) # full data\n",
        "print(\"Train MAE: \", metrics.mean_absolute_error(y_train, y_train_pred))\n",
        "print(\"Train RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
        "\n",
        "print(\"Test MAE: \", metrics.mean_absolute_error(y_test, y_test_pred))\n",
        "print(\"Test RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
        "\n",
        "print(\"Full Data MAE: \", metrics.mean_absolute_error(y, y_pred))\n",
        "print(\"Full Data RMSE: \", np.sqrt(metrics.mean_squared_error(y, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-y4H0Le_65C",
        "colab_type": "text"
      },
      "source": [
        "Lets remove recroom and stories_two has they have p value greater than 0.05 and re-run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-W1gyPO_65D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a Python list of feature names\n",
        "independent_variables = ['lotsize', 'bathrms','driveway', 'fullbase','gashw', 'airco','garagepl', \n",
        "                         'prefarea','stories_one','stories_three']\n",
        "\n",
        "# use the list to select a subset from original DataFrame\n",
        "X = df[independent_variables]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1)\n",
        "\n",
        "# create a fitted model\n",
        "lm = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# print the summary\n",
        "print(lm.summary())\n",
        "\n",
        "# make predictions on the testing set\n",
        "y_train_pred = lm.predict(X_train)\n",
        "y_test_pred = lm.predict(X_test)\n",
        "y_pred = lm.predict(X) # full data\n",
        "print(\"Train MAE: \", metrics.mean_absolute_error(y_train, y_train_pred))\n",
        "print(\"Train RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
        "\n",
        "print(\"Test MAE: \", metrics.mean_absolute_error(y_test, y_test_pred))\n",
        "print(\"Test RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
        "\n",
        "print(\"Full Data MAE: \", metrics.mean_absolute_error(y, y_pred))\n",
        "print(\"Full Data RMSE: \", np.sqrt(metrics.mean_squared_error(y, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xQey-nI_65J",
        "colab_type": "text"
      },
      "source": [
        "### Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVJLGczuKEQX",
        "colab_type": "text"
      },
      "source": [
        "# **Regression Diagnostics**\n",
        "There are a set of procedures and assumptions that need to be verified about our model\n",
        "results, otherwise the model could be misleading. Let’s look at some of the important\n",
        "regression diagnostics.\n",
        "# **Outliers**\n",
        "Data points that are far away from the fitted regression line are called outliers, and these\n",
        "can impact the accuracy of the model. Plotting normalized residual vs. leverage will give\n",
        "us a good understanding of the outlier’s points. Residual is the difference between actual\n",
        "vs. predicted, and leverage is a measure of how far away the independent variable values\n",
        "of observation are from those of the other observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrPdM1Y__65K",
        "colab_type": "text"
      },
      "source": [
        "Plotting normalized residual vs leverage to understand the outlier points.\n",
        "\n",
        "residual: difference between actual and predicted\n",
        "\n",
        "leverage: is a measure of how far away the independent variable values of an observation are from those of the other observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQnwlIbl_65K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets plot the normalized residual vs leverage\n",
        "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
        "\n",
        "ax = plt.subplots(figsize=(8,6))\n",
        "fig = plot_leverage_resid2(lm, ax = ax)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP6pJ2bcM-Jc",
        "colab_type": "text"
      },
      "source": [
        "From the chart, we see that there are many observations that have high leverage and\n",
        "residual. Running a Bonferroni outlier test will give us p-values for each observation, and\n",
        "those observations with p-value < 0.05 are the outliers affecting the accuracy. It is a good\n",
        "practice to consult or apply business domain knowledge to make a decision on removing\n",
        "the outlier points and rerunning the model; these points could be natural in the process,\n",
        "although they are mathematically found as outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGl5hFKz_65Q",
        "colab_type": "text"
      },
      "source": [
        "#### Bonferroni Outlier Test\n",
        "\n",
        "Bonferroni p-value for the most extreme observation. If the p value is < 0.05 its an outlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FqGjYvq_65Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find outliers #\n",
        "# Bonferroni outlier test\n",
        "test = lm.outlier_test()\n",
        "\n",
        "print('Bad data points (bonf(p) < 0.05):')\n",
        "print(test[test['bonf(p)'] < 0.05])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaEwVx71NR5Y",
        "colab_type": "text"
      },
      "source": [
        "Homoscedasticity and Normality\n",
        "The error variance should be constant, which is known as homoscedasticity, and the\n",
        "error should be normally distributed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJO_1T7_65U",
        "colab_type": "text"
      },
      "source": [
        "### Residual should be scattered and not have any patterns. The distribution of residual should be normal.\n",
        "\n",
        "Homoscedasticity and Normality: The error variance should be constant which is known has homoscedasticity and the error should be normally distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkw1_WIK_65Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot to check homoscedasticity\n",
        "plt.plot(lm.resid,'o')\n",
        "plt.title('Residual Plot')\n",
        "plt.ylabel('Residual')\n",
        "plt.xlabel('Observation Numbers')\n",
        "plt.show()\n",
        "\n",
        "# normality plot\n",
        "plt.hist(lm.resid, density=True)\n",
        "plt.title('Error Normality Plot')\n",
        "plt.xlabel('Residual')\n",
        "plt.ylabel('Observation Numbers')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7xri-kUPEcL",
        "colab_type": "text"
      },
      "source": [
        "The relationships between the predictors and the outcome variable should be linear.\n",
        "If the relationship is not linear then appropriate transformation (such as log, square root,\n",
        "and higher order polynomials, etc.) should be applied to the dependent/independent\n",
        "variable to fix the issue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLH-jcRF_65i",
        "colab_type": "text"
      },
      "source": [
        "You can check all the regressors using plot_partregress_grid function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGd41eFD_65i",
        "colab_type": "text"
      },
      "source": [
        "if the residual plot is not scattered or not normally distributed then there is a chance that independent variable is not having linear relationship and applying appropriate transformation to the independent variable will fix the issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHnZ4Ll4_65j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# linearity plots\n",
        "fig = plt.figure(figsize=(10,15))\n",
        "fig = sm.graphics.plot_partregress_grid(lm, fig=fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW6Z62aGPhll",
        "colab_type": "text"
      },
      "source": [
        "# **Overfitting and Underfitting**\n",
        "Underfitting occurs when the model does not fit the data well and is unable to capture\n",
        "the underlying trend in it. In this case, we can notice a low accuracy in training and test\n",
        "data set.\n",
        "Conversely, overfitting occurs when the model fits the data too well, capturing all the\n",
        "noises. In this case, we can notice a high accuracy in the training data set, whereas the\n",
        "same model will result in a low accuracy on the test data set. This means the model has\n",
        "fitted the line so well to the train data set that it failed to generalize it to fit well on the\n",
        "unseen data set. Figure 3-8 shows how the different fitting would look like for an earlier\n",
        "discussed example use case. Choice of right order polynomial degree is very important\n",
        "to avoid an overfitting or underfitting issue in regression. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LEEDfhr_65n",
        "colab_type": "text"
      },
      "source": [
        "### Regularization\n",
        "\n",
        "With increase in number of variables and increase in model complexity, the probability of over-fitting. Regularization is a technique to avoid over-fitting problem. Over-fitting occurs when the model fits the data too well capturing all the noises. In this case we can notice a high accuracy in the training data set, whereas the same model will result a low accuracy on test data set. This means the model has fitted the line so well to the train data set that it failed to generalize it to fit well on unseen data set. \n",
        "\n",
        "Statsmodel and the scikit-learn provides Ridge and LASSO(Least Absolute Shrinkage and Selection Operator) regression to handle over-fitting issue. With increase in model complexity, the size of coefficients increase exponentially, so the ridge and LASSO regression apply penalty to the magnitude of the co-efficients to handle the issue.  \n",
        "\n",
        "Ridge Regression: Also known as Tikhonov regularization, use this when you have many of variables that add minor value to the model accuracy individually, however overall improves the model accuracy and can not be excluded form the model. Ridge regression will apply penalty to reduce the magnitude of the coeffecient of all variables that add minor value to the model accuracy. Adds penalty equivalent to square of the magnitude of coefficients, alpha is the regularization strength and must be a positive float. \n",
        " \n",
        "LASSO: This provides a sparse solution, suitable when you have large set of indepenent variables. The coefficients of the variables that add minor value to the model will be zero. Adds penalty equivalent to absolute value of the magnitude of coefficients. Model complexity decreases with increase in the values of alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSI2ao7tP3HO",
        "colab_type": "text"
      },
      "source": [
        "Ridge regression: Also known as Tikhonov (L2) regularization, it guides parameters\n",
        "to be close to zero but not zero. You can use this when you have many variables that\n",
        "add minor value to the model accuracy individually, however overall improve the\n",
        "model accuracy and cannot be excluded from the model. Ridge regression will apply a\n",
        "penalty to reduce the magnitude of the coefficient of all variables that add minor value\n",
        "to the model accuracy, adding penalty equivalent to the square of the magnitude of\n",
        "coefficients. Alpha is the regularization strength and must be a positive float."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmJ8tYK3_65o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('Grade_Set_2.csv')\n",
        "df.columns = ['x','y']\n",
        "\n",
        "for i in range(2,50):              # power of 1 is already there\n",
        "    colname = 'x_%d'%i              # new var will be x_power\n",
        "    df[colname] = df['x']**i\n",
        "\n",
        "independent_variables = list(df.columns)\n",
        "independent_variables.remove('y')\n",
        "\n",
        "X= df[independent_variables]       # independent variable\n",
        "y= df.y                            # dependent variable \n",
        "\n",
        "# split data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1)\n",
        "\n",
        "# Ridge regression\n",
        "lr = linear_model.Ridge(alpha=0.001)\n",
        "lr.fit(X_train, y_train)\n",
        "y_train_pred = lr.predict(X_train)\n",
        "y_test_pred = lr.predict(X_test)\n",
        "\n",
        "print(\"------ Ridge Regression ------\")\n",
        "print(\"Train MAE: \", metrics.mean_absolute_error(y_train, y_train_pred))\n",
        "print(\"Train RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
        "\n",
        "print(\"Test MAE: \", metrics.mean_absolute_error(y_test, y_test_pred))\n",
        "print(\"Test RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
        "print(\"Ridge Coef: \", lr.coef_)\n",
        "\n",
        "# LASSO regression\n",
        "lr = linear_model.Lasso(alpha=0.001)\n",
        "lr.fit(X_train, y_train)\n",
        "y_train_pred = lr.predict(X_train)\n",
        "y_test_pred = lr.predict(X_test)\n",
        "\n",
        "print(\"----- LASSO Regression -----\")\n",
        "print(\"Train MAE: \", metrics.mean_absolute_error(y_train, y_train_pred))\n",
        "print(\"Train RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
        "\n",
        "print(\"Test MAE: \", metrics.mean_absolute_error(y_test, y_test_pred))\n",
        "print(\"Test RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
        "print(\"LASSO Coef: \", lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}